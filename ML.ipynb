{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a2dfc29",
   "metadata": {},
   "source": [
    "导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bbbd98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import spacy\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import urlextract\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier,SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96475d26",
   "metadata": {},
   "source": [
    "第一步：读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a642fb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", engine=\"python\")\n",
    "train.columns = [\"Sentiment\", \"time\", \"date\", \"query\", \"username\", \"Tweet_content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebac8aac",
   "metadata": {},
   "source": [
    "第二步：将文件中Sentiment列的4和0改为'Positive'和'Negative'（ps：不知道为什么，不改的话后面拆分会出bug读不出来分类）\n",
    "（由于数据集中的数据量过大，因此本模型中只选取每种情绪中的前两万个数据，共四万数据，同时也保证了样本量的均衡）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b082bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = train[train['Sentiment'] == 4]\n",
    "data_neg = train[train['Sentiment'] == 0]\n",
    "data_pos = data_pos.iloc[:int(20000)]\n",
    "data_neg = data_neg.iloc[:int(20000)]\n",
    "train = pd.concat([data_pos, data_neg])\n",
    "train.loc[train['Sentiment'] == 4, 'Sentiment'] = 'Positive'\n",
    "train.loc[train['Sentiment'] == 0, 'Sentiment'] = 'Negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be283743",
   "metadata": {},
   "source": [
    "第三步：EDA处理，即对文字进行预处理  （下面包括定义的各种预处理函数）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f464e7ef",
   "metadata": {},
   "source": [
    "尝试使用过下面的代码去除URLs连接，但是比起正则的方法来说运行速度太慢了：\n",
    "\n",
    "def remove_urls(text):\n",
    "\n",
    "    \"\"\"Removes URLs from the given text\"\"\"\n",
    "    extractor = urlextract.URLExtract()\n",
    "    urls = extractor.find_urls(text)\n",
    "    for url in urls:\n",
    "        text = text.replace(url, \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eac3e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用正则的方法来去除文字内容中的URL链接\n",
    "#参考文献：https://www.w3schools.com/python/python_regex.asp(介绍正则相关内容的)\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    result = url_pattern.sub(r'', text)\n",
    "    return result\n",
    "\n",
    "#使用正则的方式去除文字内容中的emoji\n",
    "#参考文献：https://unicode.org/emoji/charts/emoji-versions.html(介绍emoji相关内容的，可以找到 Emoji 表情符号的代码点范围)\n",
    "def remove_emojis(text):\n",
    "    \"\"\"Removes emojis from the given text\"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"  # dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    result = emoji_pattern.sub(r'', text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "740d9875",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Tweet_content'] = train['Tweet_content'].apply(remove_urls)\n",
    "train['Tweet_content'] = train['Tweet_content'].apply(remove_emojis)\n",
    "train['text_lens']=train['Tweet_content'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ca9b8e",
   "metadata": {},
   "source": [
    "处理完后去除数据中的缺失值和重复值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81fae627",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace=True)\n",
    "train.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8893bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除'train'数据集中指定列（'text_lens'列）中的异常值（outliers）。\n",
    "#异常值是指与其他样本明显不同或偏离正常分布的数据点，其存在可能会对数据分析和模型训练产生不良影响。\n",
    "#整个过程可以理解为在箱线图中去除超出上下限的数据点，使得数据集更加接近正常分布。\n",
    "#这有助于减少异常值对数据分析和模型训练的影响，提高模型的鲁棒性和准确性。\n",
    "#参考文献：可以搜索关于\"outlier detection\"或\"outlier removal\"的相关文献\n",
    "def remove_outlier(df_in, col_name):\n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n",
    "    return df_out\n",
    "\n",
    "#remove outliers\n",
    "train = remove_outlier(train,'text_lens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925bbd94",
   "metadata": {},
   "source": [
    "标记化和词表化：\n",
    "\n",
    "对DataFrame train 中的 'Tweet_content' 列进行文本预处理，并将预处理后的结果存储在新的 'preprocessed_text' 列中。\n",
    "\n",
    "下面这段代码将对 'Tweet_content' 列中的每个文本样本进行分词、词性标注、词干提取，并去除停用词和标点符号，最终将预处理后的文本存储在 'preprocessed_text' 列中。这些预处理操作可以减少文本中的噪声，使得后续的文本分析和建模任务更加有效和准确。\n",
    "\n",
    "唯一的问题就是运行时间较长，大概要10分钟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1adb1027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#参考文献：SpaCy文档: https://spacy.io/\n",
    "#可以在相关的自然语言处理教程和SpaCy文档中找到相关的资料和例子\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens=[]\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            filtered_tokens.append(token.lemma_)\n",
    "    return \" \".join(filtered_tokens)\n",
    "    \n",
    "\n",
    "train['preprocessed_text']=train['Tweet_content'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187be82f",
   "metadata": {},
   "source": [
    "第四步：拆分成训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8087c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train ,X_test , y_train, y_test = train_test_split(train[['preprocessed_text']],train[['Sentiment']],test_size=0.2,random_state=42,stratify=train[['Sentiment']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a611fc33",
   "metadata": {},
   "source": [
    "第五步：将原始的文本数据转换为TF-IDF特征矩阵\n",
    "\n",
    "来源于chatgpt的解释：\n",
    "\n",
    "在这个过程中，TfidfVectorizer会计算每个词的TF-IDF值，这是一种用于衡量一个词在文档中的重要性的度量。TF-IDF值越高，表示词在当前文本中越重要，但同时考虑了该词在整个语料库中的频率。\n",
    "\n",
    "结果是，我们获得了一个稀疏矩阵，其中的每一行代表一个文本数据样本，每一列代表一个词语，并且值表示对应词语的TF-IDF值。\n",
    "\n",
    "这样的转换将文本数据转换为数值特征，使得我们可以将其输入到机器学习模型中进行分类或其他任务的训练和预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0018113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#参考文献：scikit-learn官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "#NLTK官方文档：https://www.nltk.org/\n",
    "#Tfidf（Term Frequency-Inverse Document Frequency）的原理和概念\n",
    "y_train = y_train['Sentiment'].map({\"Positive\": 0, \"Negative\": 1})\n",
    "y_test = y_test['Sentiment'].map({\"Positive\": 0, \"Negative\": 1})\n",
    "\n",
    "vectorizer= TfidfVectorizer()\n",
    "\n",
    "X_train_vect= vectorizer.fit_transform(X_train['preprocessed_text'])\n",
    "X_test_vect= vectorizer.transform(X_test['preprocessed_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ca6d75",
   "metadata": {},
   "source": [
    "第六步：使用机器学习方法中的随机森林方法进行训练，这个也要大概10分钟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fa57007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.72      0.72      4000\n",
      "           1       0.72      0.73      0.72      4000\n",
      "\n",
      "    accuracy                           0.72      8000\n",
      "   macro avg       0.72      0.72      0.72      8000\n",
      "weighted avg       0.72      0.72      0.72      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_vect,y_train)\n",
    "y_predict= model.predict(X_test_vect)\n",
    "\n",
    "print(classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868cfb30",
   "metadata": {},
   "source": [
    "第七步：使用机器学习方法中的MultinomialNB方法，这个方法运行的很快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afb7cb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.68      0.71      4000\n",
      "           1       0.71      0.77      0.74      4000\n",
      "\n",
      "    accuracy                           0.72      8000\n",
      "   macro avg       0.73      0.72      0.72      8000\n",
      "weighted avg       0.73      0.72      0.72      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train_vect,y_train)\n",
    "y_predict= model.predict(X_test_vect)\n",
    "print(classification_report(y_test,y_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
